{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing image processing libraries\n",
    "import cv2  # OpenCV for image processing\n",
    "import numpy as np  # NumPy for numerical operations\n",
    "\n",
    "# Importing deep learning frameworks\n",
    "import tensorflow as tf  # TensorFlow for building and training models\n",
    "# Additional libraries for data handling and visualization\n",
    "import pandas as pd  # Pandas for data manipulation\n",
    "import matplotlib.pyplot as plt  # Matplotlib for data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15501 images with corresponding font names.\n",
      "\n",
      "Fonts:\n",
      "0: Verdana\n",
      "1: Times New Roman\n",
      "2: Californian FB\n",
      "3: Calvin\n",
      "4: Futura\n",
      "5: Minion\n",
      "6: Nasalization\n",
      "7: Algerian\n",
      "8: Bembo\n",
      "9: Akzidenz Grotesk\n",
      "10: Didot\n",
      "11: Monotype Corsiva\n",
      "12: Brandish\n",
      "13: Cambria\n",
      "14: Perpetua\n",
      "15: Calligraphy\n",
      "16: Garamond\n",
      "17: Elephant\n",
      "18: News Gothic\n",
      "19: Georgia\n",
      "20: Courier\n",
      "21: Book Antiqua\n",
      "22: Lucida Bright\n",
      "23: Mrs Eaves\n",
      "24: Comic Sans MS\n",
      "25: Snowdrift Regular\n",
      "26: Century\n",
      "27: Arial\n",
      "28: Gill sans\n",
      "29: Consolas\n",
      "30: LCD Mono\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def load_font_data(data_dir):\n",
    "\n",
    "    \n",
    "\n",
    "    image_font_pairs = []\n",
    "\n",
    "    # Loop through each subdirectory (font folder)\n",
    "    for font_dir in os.listdir(data_dir):\n",
    "        if os.path.isdir(os.path.join(data_dir, font_dir)):\n",
    "            # Get the font name\n",
    "            font_name = font_dir\n",
    "\n",
    "            # Get all image files in the font directory\n",
    "            image_files = [f for f in os.listdir(os.path.join(data_dir, font_dir)) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "\n",
    "            # Select every 2nd image starting from the first one\n",
    "            for i in range(0, len(image_files), 2):\n",
    "                # Build the image path\n",
    "                img_path = os.path.join(data_dir, font_dir, image_files[i])\n",
    "\n",
    "                # Append (image_path, font_name) tuple to the list\n",
    "                image_font_pairs.append((img_path, font_name))\n",
    "\n",
    "    return image_font_pairs\n",
    "\n",
    "# Example usage\n",
    "data_dir = r\"C:\\Users\\chaitanya\\Downloads\\week 2 day 1\\font detection\\dataset\"\n",
    "image_font_pairs = load_font_data(data_dir)\n",
    "\n",
    "print(f\"Loaded {len(image_font_pairs)} images with corresponding font names.\")\n",
    "\n",
    "# Show the actual font names\n",
    "output_labels = set([pair[1] for pair in image_font_pairs])\n",
    "\n",
    "print(\"\\nFonts:\")\n",
    "for label, font_name in enumerate(output_labels):\n",
    "    print(f\"{label}: {font_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed 15501 images with corresponding font names.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_images(image_font_pairs, target_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    Preprocesses a list of images by loading, converting to grayscale, resizing, and normalizing.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    preprocessed_images = []\n",
    "    font_names = []\n",
    "\n",
    "    for img_path, font_name in image_font_pairs:\n",
    "        # Load the image\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Check if the image is loaded successfully\n",
    "        if img is None:\n",
    "            print(f\"Error loading image: {img_path}\")\n",
    "            continue\n",
    "\n",
    "        # Resize the image to the target size\n",
    "        img = cv2.resize(img, target_size)\n",
    "\n",
    "        # Normalize the pixel values to the range [0, 1]\n",
    "        img = img / 255.0\n",
    "\n",
    "        # Add a channel dimension to make it compatible with neural network input shape\n",
    "        #img = np.expand_dims(img, axis=-1)\n",
    "\n",
    "        # Append preprocessed image and font name to the lists\n",
    "        preprocessed_images.append(img)\n",
    "        font_names.append(font_name)\n",
    "\n",
    "    return preprocessed_images, font_names\n",
    "\n",
    "# Example usage\n",
    "preprocessed_images, font_names = preprocess_images(image_font_pairs)\n",
    "\n",
    "print(f\"Preprocessed {len(preprocessed_images)} images with corresponding font names.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 10850 images\n",
      "Validation set: 2325 images\n",
      "Test set: 2326 images\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and temp sets (70% training, 30% temp)\n",
    "train_images, temp_images, train_fonts, temp_fonts = train_test_split(preprocessed_images, font_names, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split temp set into validation and test sets (50% validation, 50% test)\n",
    "val_images, test_images, val_fonts, test_fonts = train_test_split(temp_images, temp_fonts, test_size=0.5, random_state=42)\n",
    "\n",
    "# Print sizes of each set\n",
    "print(f\"Training set: {len(train_images)} images\")\n",
    "print(f\"Validation set: {len(val_images)} images\")\n",
    "print(f\"Test set: {len(test_images)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_10\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_10\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">15,903</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_40 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_40 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_41 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_41 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_42 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_42 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_43 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m295,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_43 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_10 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m524,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m)             │        \u001b[38;5;34m15,903\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">928,543</span> (3.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m928,543\u001b[0m (3.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">928,543</span> (3.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m928,543\u001b[0m (3.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Creates a convolutional neural network (CNN) model with four convolutional layers.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Convolutional layer 1\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Convolutional layer 2\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Convolutional layer 3\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Convolutional layer 4\n",
    "    model.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Flatten layer\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Fully connected layer\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "\n",
    "    # Dropout regularization\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define input shape and number of classes\n",
    "input_shape = (64, 64, 1)  \n",
    "num_classes = 31 # 31 different font classes\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create CNN model\n",
    "cnn_model = create_cnn_model(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "cnn_model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "cnn_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform labels for training data\n",
    "train_labels_encoded = label_encoder.fit_transform(train_fonts)\n",
    "\n",
    "# Transform labels for validation and test data\n",
    "val_labels_encoded = label_encoder.transform(val_fonts)\n",
    "test_labels_encoded = label_encoder.transform(test_fonts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Convert train_images, val_images, test_images to NumPy arrays\n",
    "train_images_np = np.array(train_images)\n",
    "val_images_np = np.array(val_images)\n",
    "test_images_np = np.array(test_images)\n",
    "\n",
    "# Convert train_fonts, val_fonts, test_fonts to NumPy arrays\n",
    "train_fonts_np = np.array(train_labels_encoded)\n",
    "val_fonts_np = np.array(val_labels_encoded)\n",
    "test_fonts_np = np.array(test_labels_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 53ms/step - accuracy: 0.0374 - loss: 3.4253 - val_accuracy: 0.1256 - val_loss: 2.9915\n",
      "Epoch 2/20\n",
      "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 49ms/step - accuracy: 0.1828 - loss: 2.7194 - val_accuracy: 0.4740 - val_loss: 1.5803\n",
      "Epoch 3/20\n",
      "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 49ms/step - accuracy: 0.4660 - loss: 1.6253 - val_accuracy: 0.5772 - val_loss: 1.2046\n",
      "Epoch 4/20\n",
      "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 48ms/step - accuracy: 0.6237 - loss: 1.1209 - val_accuracy: 0.7708 - val_loss: 0.7126\n",
      "Epoch 5/20\n",
      "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 47ms/step - accuracy: 0.7289 - loss: 0.8198 - val_accuracy: 0.7802 - val_loss: 0.6555\n",
      "Epoch 6/20\n",
      "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 49ms/step - accuracy: 0.7840 - loss: 0.6597 - val_accuracy: 0.8705 - val_loss: 0.4417\n",
      "Epoch 7/20\n",
      "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 48ms/step - accuracy: 0.8227 - loss: 0.5321 - val_accuracy: 0.8826 - val_loss: 0.3550\n",
      "Epoch 8/20\n",
      "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 48ms/step - accuracy: 0.8530 - loss: 0.4312 - val_accuracy: 0.9110 - val_loss: 0.3047\n",
      "Epoch 9/20\n",
      "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 50ms/step - accuracy: 0.8763 - loss: 0.3789 - val_accuracy: 0.9187 - val_loss: 0.2583\n",
      "Epoch 10/20\n",
      "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 47ms/step - accuracy: 0.8912 - loss: 0.3310 - val_accuracy: 0.9028 - val_loss: 0.3006\n",
      "Epoch 11/20\n",
      "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 48ms/step - accuracy: 0.9037 - loss: 0.3001 - val_accuracy: 0.9342 - val_loss: 0.2049\n",
      "Epoch 12/20\n",
      "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 50ms/step - accuracy: 0.9140 - loss: 0.2581 - val_accuracy: 0.9389 - val_loss: 0.1905\n",
      "Epoch 13/20\n",
      "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 47ms/step - accuracy: 0.9172 - loss: 0.2482 - val_accuracy: 0.9363 - val_loss: 0.2264\n",
      "Epoch 14/20\n",
      "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 46ms/step - accuracy: 0.9345 - loss: 0.2009 - val_accuracy: 0.9441 - val_loss: 0.2090\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9402 - loss: 0.1848\n",
      "Test Loss: 0.2061142474412918\n",
      "Test Accuracy: 0.935511589050293\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "\n",
    "# Assuming your model is already defined\n",
    "\n",
    "# Prepare callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)  # Stop training if val_loss doesn't improve for 2 epochs\n",
    "model_checkpoint = ModelCheckpoint('C:/Users/chaitanya/Downloads/week 2 day 1/best_model.keras', save_best_only=True)  # Save the best model based on val_loss\n",
    "\n",
    "\n",
    "# Train the model with callbacks\n",
    "history = cnn_model.fit(train_images_np, train_fonts_np, epochs=20,\n",
    "                        validation_data=(val_images_np, val_fonts_np),\n",
    "                        callbacks=[early_stopping, model_checkpoint])  # Pass callbacks as a list\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = cnn_model.evaluate(test_images_np, test_fonts_np)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96        88\n",
      "           1       0.94      0.98      0.96        64\n",
      "           2       0.93      0.96      0.94        67\n",
      "           3       0.98      0.72      0.83        78\n",
      "           4       0.93      0.91      0.92        81\n",
      "           5       0.95      1.00      0.97        78\n",
      "           6       0.92      0.96      0.94        82\n",
      "           7       0.89      0.96      0.92        70\n",
      "           8       0.99      0.93      0.96        73\n",
      "           9       0.83      0.88      0.86        77\n",
      "          10       0.99      0.95      0.97        84\n",
      "          11       0.89      1.00      0.94        72\n",
      "          12       0.90      1.00      0.94        77\n",
      "          13       0.99      0.99      0.99        72\n",
      "          14       0.97      0.97      0.97        61\n",
      "          15       0.95      0.97      0.96        71\n",
      "          16       0.99      0.94      0.96        79\n",
      "          17       0.98      0.84      0.91        74\n",
      "          18       0.95      0.92      0.93        83\n",
      "          19       0.87      0.94      0.90        82\n",
      "          20       1.00      0.97      0.99        74\n",
      "          21       0.99      0.97      0.98        78\n",
      "          22       0.74      0.96      0.84        70\n",
      "          23       0.93      0.88      0.90        65\n",
      "          24       0.96      0.92      0.94        74\n",
      "          25       0.98      0.98      0.98        81\n",
      "          26       0.91      0.88      0.89        67\n",
      "          27       1.00      0.95      0.97        77\n",
      "          28       1.00      1.00      1.00        74\n",
      "          29       0.97      0.80      0.88        76\n",
      "          30       0.83      0.97      0.90        77\n",
      "\n",
      "    accuracy                           0.94      2326\n",
      "   macro avg       0.94      0.94      0.94      2326\n",
      "weighted avg       0.94      0.94      0.94      2326\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Calculate predicted labels for the test set\n",
    "predicted_labels = np.argmax(cnn_model.predict(test_images_np), axis=1)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_fonts_np, predicted_labels)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
