{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing image processing libraries\n",
    "import cv2  # OpenCV for image processing\n",
    "import numpy as np  # NumPy for numerical operations\n",
    "\n",
    "# Importing deep learning frameworks\n",
    "import tensorflow as tf  # TensorFlow for building and training models\n",
    "# Additional libraries for data handling and visualization\n",
    "import pandas as pd  # Pandas for data manipulation\n",
    "import matplotlib.pyplot as plt  # Matplotlib for data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 16001 images with corresponding font names.\n",
      "\n",
      "Fonts:\n",
      "0: Verdana\n",
      "1: Times New Roman\n",
      "2: Hombre\n",
      "3: Californian FB\n",
      "4: Calvin\n",
      "5: Futura\n",
      "6: Minion\n",
      "7: Nasalization\n",
      "8: Algerian\n",
      "9: Bembo\n",
      "10: Akzidenz Grotesk\n",
      "11: Didot\n",
      "12: Monotype Corsiva\n",
      "13: Brandish\n",
      "14: Cambria\n",
      "15: Perpetua\n",
      "16: Calligraphy\n",
      "17: Garamond\n",
      "18: Elephant\n",
      "19: News Gothic\n",
      "20: Georgia\n",
      "21: Courier\n",
      "22: Book Antiqua\n",
      "23: Lucida Bright\n",
      "24: Mrs Eaves\n",
      "25: Comic Sans MS\n",
      "26: Snowdrift Regular\n",
      "27: Century\n",
      "28: Arial\n",
      "29: Gill sans\n",
      "30: Consolas\n",
      "31: LCD Mono\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def load_font_data(data_dir):\n",
    "\n",
    "    \n",
    "\n",
    "    image_font_pairs = []\n",
    "\n",
    "    # Loop through each subdirectory (font folder)\n",
    "    for font_dir in os.listdir(data_dir):\n",
    "        if os.path.isdir(os.path.join(data_dir, font_dir)):\n",
    "            # Get the font name\n",
    "            font_name = font_dir\n",
    "\n",
    "            # Get all image files in the font directory\n",
    "            image_files = [f for f in os.listdir(os.path.join(data_dir, font_dir)) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "\n",
    "            # Select every 2nd image starting from the first one\n",
    "            for i in range(0, len(image_files), 2):\n",
    "                # Build the image path\n",
    "                img_path = os.path.join(data_dir, font_dir, image_files[i])\n",
    "\n",
    "                # Append (image_path, font_name) tuple to the list\n",
    "                image_font_pairs.append((img_path, font_name))\n",
    "\n",
    "    return image_font_pairs\n",
    "\n",
    "# Example usage\n",
    "data_dir = r\"C:\\Users\\chaitanya\\Downloads\\week 2 day 1\\font detection\\dataset\"\n",
    "image_font_pairs = load_font_data(data_dir)\n",
    "\n",
    "print(f\"Loaded {len(image_font_pairs)} images with corresponding font names.\")\n",
    "\n",
    "# Show the actual font names\n",
    "output_labels = set([pair[1] for pair in image_font_pairs])\n",
    "\n",
    "print(\"\\nFonts:\")\n",
    "for label, font_name in enumerate(output_labels):\n",
    "    print(f\"{label}: {font_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed 16001 images with corresponding font names.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_images(image_font_pairs, target_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    Preprocesses a list of images by loading, converting to grayscale, resizing, and normalizing.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    preprocessed_images = []\n",
    "    font_names = []\n",
    "\n",
    "    for img_path, font_name in image_font_pairs:\n",
    "        # Load the image\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Check if the image is loaded successfully\n",
    "        if img is None:\n",
    "            print(f\"Error loading image: {img_path}\")\n",
    "            continue\n",
    "\n",
    "        # Resize the image to the target size\n",
    "        img = cv2.resize(img, target_size)\n",
    "\n",
    "        # Normalize the pixel values to the range [0, 1]\n",
    "        img = img / 255.0\n",
    "\n",
    "        # Add a channel dimension to make it compatible with neural network input shape\n",
    "        #img = np.expand_dims(img, axis=-1)\n",
    "\n",
    "        # Append preprocessed image and font name to the lists\n",
    "        preprocessed_images.append(img)\n",
    "        font_names.append(font_name)\n",
    "\n",
    "    return preprocessed_images, font_names\n",
    "\n",
    "# Example usage\n",
    "preprocessed_images, font_names = preprocess_images(image_font_pairs)\n",
    "\n",
    "print(f\"Preprocessed {len(preprocessed_images)} images with corresponding font names.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 11200 images\n",
      "Validation set: 2400 images\n",
      "Test set: 2401 images\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and temp sets (70% training, 30% temp)\n",
    "train_images, temp_images, train_fonts, temp_fonts = train_test_split(preprocessed_images, font_names, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split temp set into validation and test sets (50% validation, 50% test)\n",
    "val_images, test_images, val_fonts, test_fonts = train_test_split(temp_images, temp_fonts, test_size=0.5, random_state=42)\n",
    "\n",
    "# Print sizes of each set\n",
    "print(f\"Training set: {len(train_images)} images\")\n",
    "print(f\"Validation set: {len(val_images)} images\")\n",
    "print(f\"Test set: {len(test_images)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chaitanya\\anaconda34\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,416</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_16 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_16 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_17 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_17 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_18 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_18 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_19 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m295,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_19 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m524,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m16,416\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">929,056</span> (3.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m929,056\u001b[0m (3.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">929,056</span> (3.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m929,056\u001b[0m (3.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Creates a convolutional neural network (CNN) model with four convolutional layers.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Convolutional layer 1\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Convolutional layer 2\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Convolutional layer 3\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Convolutional layer 4\n",
    "    model.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Flatten layer\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Fully connected layer\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "\n",
    "    # Dropout regularization\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define input shape and number of classes\n",
    "input_shape = (64, 64, 1)  \n",
    "num_classes = 32  # 32 different font classes\n",
    "tf.random.set_seed(64)\n",
    "\n",
    "# Create CNN model\n",
    "cnn_model = create_cnn_model(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "cnn_model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "cnn_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform labels for training data\n",
    "train_labels_encoded = label_encoder.fit_transform(train_fonts)\n",
    "\n",
    "# Transform labels for validation and test data\n",
    "val_labels_encoded = label_encoder.transform(val_fonts)\n",
    "test_labels_encoded = label_encoder.transform(test_fonts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Convert train_images, val_images, test_images to NumPy arrays\n",
    "train_images_np = np.array(train_images)\n",
    "val_images_np = np.array(val_images)\n",
    "test_images_np = np.array(test_images)\n",
    "\n",
    "# Convert train_fonts, val_fonts, test_fonts to NumPy arrays\n",
    "train_fonts_np = np.array(train_labels_encoded)\n",
    "val_fonts_np = np.array(val_labels_encoded)\n",
    "test_fonts_np = np.array(test_labels_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 46ms/step - accuracy: 0.0321 - loss: 3.4638 - val_accuracy: 0.1163 - val_loss: 3.0704\n",
      "Epoch 2/20\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 49ms/step - accuracy: 0.1972 - loss: 2.7013 - val_accuracy: 0.4571 - val_loss: 1.6736\n",
      "Epoch 3/20\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 46ms/step - accuracy: 0.4831 - loss: 1.5751 - val_accuracy: 0.6971 - val_loss: 0.9428\n",
      "Epoch 4/20\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 44ms/step - accuracy: 0.6120 - loss: 1.1448 - val_accuracy: 0.7021 - val_loss: 0.8229\n",
      "Epoch 5/20\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 45ms/step - accuracy: 0.7197 - loss: 0.8021 - val_accuracy: 0.8196 - val_loss: 0.5249\n",
      "Epoch 6/20\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 46ms/step - accuracy: 0.7851 - loss: 0.6360 - val_accuracy: 0.8042 - val_loss: 0.5522\n",
      "Epoch 7/20\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 49ms/step - accuracy: 0.8137 - loss: 0.5411 - val_accuracy: 0.8717 - val_loss: 0.3705\n",
      "Epoch 8/20\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 47ms/step - accuracy: 0.8480 - loss: 0.4385 - val_accuracy: 0.8533 - val_loss: 0.4303\n",
      "Epoch 9/20\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 45ms/step - accuracy: 0.8621 - loss: 0.4099 - val_accuracy: 0.9121 - val_loss: 0.2599\n",
      "Epoch 10/20\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 45ms/step - accuracy: 0.8867 - loss: 0.3381 - val_accuracy: 0.9350 - val_loss: 0.2040\n",
      "Epoch 11/20\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 44ms/step - accuracy: 0.9064 - loss: 0.2842 - val_accuracy: 0.9346 - val_loss: 0.2058\n",
      "Epoch 12/20\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 44ms/step - accuracy: 0.9152 - loss: 0.2480 - val_accuracy: 0.9438 - val_loss: 0.1903\n",
      "Epoch 13/20\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 44ms/step - accuracy: 0.9213 - loss: 0.2298 - val_accuracy: 0.9371 - val_loss: 0.1933\n",
      "Epoch 14/20\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 45ms/step - accuracy: 0.9319 - loss: 0.2082 - val_accuracy: 0.9200 - val_loss: 0.2506\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9266 - loss: 0.2706\n",
      "Test Loss: 0.2495134174823761\n",
      "Test Accuracy: 0.9241982698440552\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "\n",
    "# Assuming your model is already defined\n",
    "\n",
    "# Prepare callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)  # Stop training if val_loss doesn't improve for 2 epochs\n",
    "model_checkpoint = ModelCheckpoint('C:/Users/chaitanya/Downloads/week 2 day 1/best_model.keras', save_best_only=True)  # Save the best model based on val_loss\n",
    "\n",
    "\n",
    "# Train the model with callbacks\n",
    "history = cnn_model.fit(train_images_np, train_fonts_np, epochs=20,\n",
    "                        validation_data=(val_images_np, val_fonts_np),\n",
    "                        callbacks=[early_stopping, model_checkpoint])  # Pass callbacks as a list\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = cnn_model.evaluate(test_images_np, test_fonts_np)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98        90\n",
      "           1       0.91      1.00      0.95        71\n",
      "           2       0.99      0.92      0.95        73\n",
      "           3       0.87      0.97      0.92        71\n",
      "           4       0.96      0.68      0.79        77\n",
      "           5       0.95      0.96      0.96        57\n",
      "           6       0.86      0.94      0.90        81\n",
      "           7       0.93      0.96      0.94        98\n",
      "           8       1.00      0.90      0.95        84\n",
      "           9       0.89      0.85      0.87        73\n",
      "          10       0.97      0.90      0.94        80\n",
      "          11       0.97      0.97      0.97        74\n",
      "          12       0.97      0.99      0.98        69\n",
      "          13       0.96      0.96      0.96        70\n",
      "          14       0.90      0.96      0.93        67\n",
      "          15       0.97      0.87      0.92        86\n",
      "          16       0.90      0.95      0.92        82\n",
      "          17       0.96      0.97      0.97        80\n",
      "          18       0.89      0.93      0.91        69\n",
      "          19       0.94      0.90      0.92        91\n",
      "          20       1.00      0.96      0.98        69\n",
      "          21       0.98      1.00      0.99        58\n",
      "          22       0.94      0.90      0.92        69\n",
      "          23       0.88      0.58      0.70        65\n",
      "          24       0.77      0.92      0.84        74\n",
      "          25       0.75      0.99      0.85        80\n",
      "          26       1.00      0.99      0.99        68\n",
      "          27       0.97      0.96      0.97        76\n",
      "          28       0.97      0.94      0.95        88\n",
      "          29       0.99      1.00      0.99        66\n",
      "          30       0.84      0.86      0.85        73\n",
      "          31       0.86      0.90      0.88        72\n",
      "\n",
      "    accuracy                           0.92      2401\n",
      "   macro avg       0.93      0.92      0.92      2401\n",
      "weighted avg       0.93      0.92      0.92      2401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Calculate predicted labels for the test set\n",
    "predicted_labels = np.argmax(cnn_model.predict(test_images_np), axis=1)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_fonts_np, predicted_labels)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
