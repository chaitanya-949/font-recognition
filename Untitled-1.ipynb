{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing image processing libraries\n",
    "import cv2  # OpenCV for image processing\n",
    "import numpy as np  # NumPy for numerical operations\n",
    "\n",
    "# Importing deep learning frameworks\n",
    "import tensorflow as tf  # TensorFlow for building and training models\n",
    "# Additional libraries for data handling and visualization\n",
    "import pandas as pd  # Pandas for data manipulation\n",
    "import matplotlib.pyplot as plt  # Matplotlib for data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15001 images with corresponding font names.\n",
      "\n",
      "Fonts:\n",
      "0: Verdana\n",
      "1: Times New Roman\n",
      "2: Californian FB\n",
      "3: Calvin\n",
      "4: Futura\n",
      "5: Minion\n",
      "6: Algerian\n",
      "7: Bembo\n",
      "8: Akzidenz Grotesk\n",
      "9: Didot\n",
      "10: Monotype Corsiva\n",
      "11: Brandish\n",
      "12: Cambria\n",
      "13: Perpetua\n",
      "14: Calligraphy\n",
      "15: Garamond\n",
      "16: Elephant\n",
      "17: News Gothic\n",
      "18: Georgia\n",
      "19: Courier\n",
      "20: Book Antiqua\n",
      "21: Lucida Bright\n",
      "22: Mrs Eaves\n",
      "23: Comic Sans MS\n",
      "24: Snowdrift Regular\n",
      "25: Century\n",
      "26: Arial\n",
      "27: Gill sans\n",
      "28: Consolas\n",
      "29: LCD Mono\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def load_font_data(data_dir):\n",
    "\n",
    "    \n",
    "\n",
    "    image_font_pairs = []\n",
    "\n",
    "    # Loop through each subdirectory (font folder)\n",
    "    for font_dir in os.listdir(data_dir):\n",
    "        if os.path.isdir(os.path.join(data_dir, font_dir)):\n",
    "            # Get the font name\n",
    "            font_name = font_dir\n",
    "\n",
    "            # Get all image files in the font directory\n",
    "            image_files = [f for f in os.listdir(os.path.join(data_dir, font_dir)) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "\n",
    "            # Select every 2nd image starting from the first one\n",
    "            for i in range(0, len(image_files), 2):\n",
    "                # Build the image path\n",
    "                img_path = os.path.join(data_dir, font_dir, image_files[i])\n",
    "\n",
    "                # Append (image_path, font_name) tuple to the list\n",
    "                image_font_pairs.append((img_path, font_name))\n",
    "\n",
    "    return image_font_pairs\n",
    "\n",
    "# Example usage\n",
    "data_dir = r\"C:\\Users\\chaitanya\\Downloads\\week 2 day 1\\font detection\\dataset\"\n",
    "image_font_pairs = load_font_data(data_dir)\n",
    "\n",
    "print(f\"Loaded {len(image_font_pairs)} images with corresponding font names.\")\n",
    "\n",
    "# Show the actual font names\n",
    "output_labels = set([pair[1] for pair in image_font_pairs])\n",
    "\n",
    "print(\"\\nFonts:\")\n",
    "for label, font_name in enumerate(output_labels):\n",
    "    print(f\"{label}: {font_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed 15001 images with corresponding font names.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_images(image_font_pairs, target_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    Preprocesses a list of images by loading, converting to grayscale, resizing, and normalizing.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    preprocessed_images = []\n",
    "    font_names = []\n",
    "\n",
    "    for img_path, font_name in image_font_pairs:\n",
    "        # Load the image\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Check if the image is loaded successfully\n",
    "        if img is None:\n",
    "            print(f\"Error loading image: {img_path}\")\n",
    "            continue\n",
    "\n",
    "        # Resize the image to the target size\n",
    "        img = cv2.resize(img, target_size)\n",
    "\n",
    "        # Normalize the pixel values to the range [0, 1]\n",
    "        img = img / 255.0\n",
    "\n",
    "        # Add a channel dimension to make it compatible with neural network input shape\n",
    "        #img = np.expand_dims(img, axis=-1)\n",
    "\n",
    "        # Append preprocessed image and font name to the lists\n",
    "        preprocessed_images.append(img)\n",
    "        font_names.append(font_name)\n",
    "\n",
    "    return preprocessed_images, font_names\n",
    "\n",
    "# Example usage\n",
    "preprocessed_images, font_names = preprocess_images(image_font_pairs)\n",
    "\n",
    "print(f\"Preprocessed {len(preprocessed_images)} images with corresponding font names.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 10500 images\n",
      "Validation set: 2250 images\n",
      "Test set: 2251 images\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and temp sets (70% training, 30% temp)\n",
    "train_images, temp_images, train_fonts, temp_fonts = train_test_split(preprocessed_images, font_names, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split temp set into validation and test sets (50% validation, 50% test)\n",
    "val_images, test_images, val_fonts, test_fonts = train_test_split(temp_images, temp_fonts, test_size=0.5, random_state=42)\n",
    "\n",
    "# Print sizes of each set\n",
    "print(f\"Training set: {len(train_images)} images\")\n",
    "print(f\"Validation set: {len(val_images)} images\")\n",
    "print(f\"Test set: {len(test_images)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chaitanya\\anaconda34\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_11\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_11\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">15,390</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_44 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_44 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_45 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_45 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_46 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_46 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_47 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m295,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_47 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_11 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m524,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)             │        \u001b[38;5;34m15,390\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">928,030</span> (3.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m928,030\u001b[0m (3.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">928,030</span> (3.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m928,030\u001b[0m (3.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Creates a convolutional neural network (CNN) model with four convolutional layers.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Convolutional layer 1\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Convolutional layer 2\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Convolutional layer 3\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Convolutional layer 4\n",
    "    model.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Flatten layer\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Fully connected layer\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "\n",
    "    # Dropout regularization\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define input shape and number of classes\n",
    "input_shape = (64, 64, 1)  \n",
    "num_classes = 30 # 30 different font classes\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create CNN model\n",
    "cnn_model = create_cnn_model(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "cnn_model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "cnn_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform labels for training data\n",
    "train_labels_encoded = label_encoder.fit_transform(train_fonts)\n",
    "\n",
    "# Transform labels for validation and test data\n",
    "val_labels_encoded = label_encoder.transform(val_fonts)\n",
    "test_labels_encoded = label_encoder.transform(test_fonts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Convert train_images, val_images, test_images to NumPy arrays\n",
    "train_images_np = np.array(train_images)\n",
    "val_images_np = np.array(val_images)\n",
    "test_images_np = np.array(test_images)\n",
    "\n",
    "# Convert train_fonts, val_fonts, test_fonts to NumPy arrays\n",
    "train_fonts_np = np.array(train_labels_encoded)\n",
    "val_fonts_np = np.array(val_labels_encoded)\n",
    "test_fonts_np = np.array(test_labels_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 52ms/step - accuracy: 0.0335 - loss: 3.4052 - val_accuracy: 0.0289 - val_loss: 3.4022\n",
      "Epoch 2/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 48ms/step - accuracy: 0.0671 - loss: 3.2549 - val_accuracy: 0.5129 - val_loss: 1.6669\n",
      "Epoch 3/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 46ms/step - accuracy: 0.5085 - loss: 1.5238 - val_accuracy: 0.7440 - val_loss: 0.8276\n",
      "Epoch 4/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 52ms/step - accuracy: 0.7424 - loss: 0.8091 - val_accuracy: 0.8622 - val_loss: 0.4943\n",
      "Epoch 5/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 49ms/step - accuracy: 0.8244 - loss: 0.5473 - val_accuracy: 0.9044 - val_loss: 0.3620\n",
      "Epoch 6/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 48ms/step - accuracy: 0.8723 - loss: 0.3978 - val_accuracy: 0.9196 - val_loss: 0.3043\n",
      "Epoch 7/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 49ms/step - accuracy: 0.8884 - loss: 0.3245 - val_accuracy: 0.9347 - val_loss: 0.2344\n",
      "Epoch 8/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 50ms/step - accuracy: 0.9065 - loss: 0.2781 - val_accuracy: 0.9378 - val_loss: 0.2291\n",
      "Epoch 9/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 48ms/step - accuracy: 0.9233 - loss: 0.2362 - val_accuracy: 0.9444 - val_loss: 0.2042\n",
      "Epoch 10/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 49ms/step - accuracy: 0.9332 - loss: 0.2055 - val_accuracy: 0.9467 - val_loss: 0.2007\n",
      "Epoch 11/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 48ms/step - accuracy: 0.9417 - loss: 0.1797 - val_accuracy: 0.9489 - val_loss: 0.2047\n",
      "Epoch 12/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 51ms/step - accuracy: 0.9428 - loss: 0.1701 - val_accuracy: 0.9556 - val_loss: 0.1724\n",
      "Epoch 13/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 49ms/step - accuracy: 0.9473 - loss: 0.1515 - val_accuracy: 0.9467 - val_loss: 0.2023\n",
      "Epoch 14/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 48ms/step - accuracy: 0.9550 - loss: 0.1416 - val_accuracy: 0.9622 - val_loss: 0.1481\n",
      "Epoch 15/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 55ms/step - accuracy: 0.9544 - loss: 0.1329 - val_accuracy: 0.9587 - val_loss: 0.1557\n",
      "Epoch 16/20\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 58ms/step - accuracy: 0.9576 - loss: 0.1256 - val_accuracy: 0.9596 - val_loss: 0.1493\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9609 - loss: 0.1472\n",
      "Test Loss: 0.13748595118522644\n",
      "Test Accuracy: 0.9631274938583374\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "\n",
    "# Assuming your model is already defined\n",
    "\n",
    "# Prepare callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)  # Stop training if val_loss doesn't improve for 2 epochs\n",
    "model_checkpoint = ModelCheckpoint('C:/Users/chaitanya/Downloads/week 2 day 1/best_model.keras', save_best_only=True)  # Save the best model based on val_loss\n",
    "\n",
    "\n",
    "# Train the model with callbacks\n",
    "history = cnn_model.fit(train_images_np, train_fonts_np, epochs=20,\n",
    "                        validation_data=(val_images_np, val_fonts_np),\n",
    "                        callbacks=[early_stopping, model_checkpoint])  # Pass callbacks as a list\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = cnn_model.evaluate(test_images_np, test_fonts_np)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        86\n",
      "           1       0.94      0.97      0.96        69\n",
      "           2       0.96      0.96      0.96        73\n",
      "           3       0.96      0.96      0.96        81\n",
      "           4       0.95      0.95      0.95        76\n",
      "           5       0.91      0.98      0.94        82\n",
      "           6       0.97      0.97      0.97        86\n",
      "           7       0.98      0.96      0.97        84\n",
      "           8       0.99      0.96      0.97        78\n",
      "           9       0.88      0.93      0.91        73\n",
      "          10       0.97      0.99      0.98        71\n",
      "          11       0.94      0.99      0.96        75\n",
      "          12       0.97      0.98      0.98        65\n",
      "          13       0.97      0.95      0.96        66\n",
      "          14       0.99      0.99      0.99        76\n",
      "          15       0.97      0.97      0.97        77\n",
      "          16       0.96      0.99      0.97        74\n",
      "          17       1.00      0.98      0.99        81\n",
      "          18       0.99      0.97      0.98        76\n",
      "          19       0.93      0.98      0.95        85\n",
      "          20       1.00      1.00      1.00        74\n",
      "          21       0.96      0.97      0.97        80\n",
      "          22       0.97      0.97      0.97        72\n",
      "          23       1.00      0.88      0.94        75\n",
      "          24       0.97      0.97      0.97        62\n",
      "          25       0.93      0.97      0.95        71\n",
      "          26       1.00      0.94      0.97        71\n",
      "          27       0.98      0.93      0.96        70\n",
      "          28       0.98      0.87      0.92        68\n",
      "          29       0.90      0.97      0.94        74\n",
      "\n",
      "    accuracy                           0.96      2251\n",
      "   macro avg       0.96      0.96      0.96      2251\n",
      "weighted avg       0.96      0.96      0.96      2251\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Calculate predicted labels for the test set\n",
    "predicted_labels = np.argmax(cnn_model.predict(test_images_np), axis=1)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_fonts_np, predicted_labels)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
